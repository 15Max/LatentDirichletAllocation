{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedded Topic Model (ETM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "from octis.preprocessing.preprocessing import Preprocessing\n",
    "from octis.models.ETM import ETM\n",
    "from octis.dataset.dataset import Dataset\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "from octis.optimization.optimizer import Optimizer\n",
    "from skopt.space.space import Real\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "os.chdir(parent_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.embeddings import *\n",
    "from preprocessing.clean_text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/raw/cleaned_train_lyrics.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.drop(columns = ['Unnamed: 0'])\n",
    "df = df.rename(columns = {'Lyric':'lyrics'})\n",
    "df = df.sample(frac=0.0001).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_file = 'data/input/embeddings.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first value of the metadata, containing the number of documents form dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings not found, generating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marta\\miniconda3\\envs\\OCTIS\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Embeddings saved to data/input/embeddings.pkl\n"
     ]
    }
   ],
   "source": [
    "embeddings = create_embeddings(df, embeddings_file, force_creation=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the preprocessing \n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus has been saved to data/input\\corpus.txt\n",
      "Labels have been saved to data/input\\labels.txt\n"
     ]
    }
   ],
   "source": [
    "extract_corpus_and_labels_from_songs_csv(csv_input_path = 'data/raw/cleaned_train_lyrics.csv', output_path = 'data/input',frac=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = list(nlp.Defaults.stop_words)\n",
    "stopwords_list.extend(CUSTOM_STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessing(lowercase=True,\n",
    "                             min_df = 10,\n",
    "                             max_df = 0.85,\n",
    "                             remove_punctuation=True,\n",
    "                             punctuation=string.punctuation,\n",
    "                             remove_numbers=True,\n",
    "                             lemmatize= True,\n",
    "                             stopword_list=stopwords_list,\n",
    "                             min_chars=3,\n",
    "                             min_words_docs=10,\n",
    "                             language='english',\n",
    "                             split=True,\n",
    "                             verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Checks if dataset folder exists in processed, if not it processes the dataset. Otherwise it loads it\n",
    "if not os.path.exists('data/processed/dataset'):\n",
    "    dataset = preprocessor.preprocess_dataset(documents_path = 'data/input/corpus.txt',labels_path = 'data/input/labels.txt')\n",
    "    dataset.save('data/processed/dataset')\n",
    "else:\n",
    "    dataset = Dataset()\n",
    "    dataset.load_custom_dataset_from_folder('data/processed/dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_documents': 50,\n",
       " 'vocabulary_length': 20,\n",
       " 'preprocessing-info': ['lowercase',\n",
       "  'remove_punctuation',\n",
       "  'lemmatize',\n",
       "  'filter words with document frequency lower than 10 and higher than 0.85',\n",
       "  'filter words with less than 3 character',\n",
       "  'filter documents with less than 10 words'],\n",
       " 'last-training-doc': 15,\n",
       " 'last-validation-doc': 19}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETM model without optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOPICS = 5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = ETM(num_topics = 10, vocab_size=3000, t_hidden_size=800, theta_act = 'relu', embeddings = None, train_embeddings = True, enc_drop = 0.0, rho_size= 5, emb_size= 10)7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ETM(num_topics= N_TOPICS,  \n",
    "        num_epochs=100, \n",
    "        t_hidden_size=800, \n",
    "        rho_size=384, \n",
    "        embedding_size=384, \n",
    "        activation='relu', \n",
    "        dropout=0.5, \n",
    "        lr=0.005, \n",
    "        optimizer='adam', \n",
    "        batch_size=128, \n",
    "        clip=0.0, \n",
    "        wdecay=1.2e-6, \n",
    "        bow_norm=1, \n",
    "        device=device, \n",
    "        train_embeddings=False, \n",
    "        embeddings_path=embeddings_file, \n",
    "        embeddings_type='pickle', \n",
    "        binary_embeddings=True, \n",
    "        headerless_embeddings=False, \n",
    "        use_partitions=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = pickle.load(open(embeddings_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (50, 768)\n"
     ]
    }
   ],
   "source": [
    "print('Embeddings shape:', embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.83585376 -0.25974375 -0.216797   ... -0.18789074 -0.49326912\n",
      "  -0.9179299 ]\n",
      " [ 0.22354187  0.21200508  0.7886475  ... -0.47905082 -0.33619213\n",
      "  -0.31313756]\n",
      " [ 1.3479398  -0.01886051 -0.16627687 ... -0.47807914  0.10327217\n",
      "  -0.8390184 ]\n",
      " ...\n",
      " [ 0.72178626  0.86839515 -0.08751168 ... -0.17257741  0.34163907\n",
      "  -0.7903824 ]\n",
      " [ 0.5833137   0.27970174  0.4332806  ...  0.0051777  -0.24891414\n",
      "  -0.58246946]\n",
      " [ 0.4199069  -0.11670721 -0.3256204  ... -0.11589666 -0.6577412\n",
      "  -0.7278601 ]]\n"
     ]
    }
   ],
   "source": [
    "print(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marta\\miniconda3\\envs\\OCTIS\\lib\\site-packages\\octis\\models\\ETM.py:87\u001b[0m, in \u001b[0;36mETM.train_model\u001b[1;34m(self, dataset, hyperparameters, top_words, op_path)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hyperparameters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m     hyperparameters \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 87\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_words \u001b[38;5;241m=\u001b[39m top_words\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(\n\u001b[0;32m     90\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, path\u001b[38;5;241m=\u001b[39mop_path)\n",
      "File \u001b[1;32mc:\\Users\\marta\\miniconda3\\envs\\OCTIS\\lib\\site-packages\\octis\\models\\ETM.py:138\u001b[0m, in \u001b[0;36mETM.set_model\u001b[1;34m(self, dataset, hyperparameters)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_default_hyperparameters(hyperparameters)\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# define model and optimizer\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m etm\u001b[38;5;241m.\u001b[39mETM(\n\u001b[0;32m    141\u001b[0m     num_topics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_topics\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    142\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mkeys()),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    148\u001b[0m     train_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    149\u001b[0m     enc_drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\marta\\miniconda3\\envs\\OCTIS\\lib\\site-packages\\octis\\models\\base_etm.py:59\u001b[0m, in \u001b[0;36mBaseETM.load_embeddings\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m vectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_word_vectors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membeddings_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membeddings_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbinary_embeddings\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheaderless_embeddings\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[0;32m     65\u001b[0m     (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mkeys()), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding_size\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mvalues()):\n",
      "File \u001b[1;32mc:\\Users\\marta\\miniconda3\\envs\\OCTIS\\lib\\site-packages\\octis\\models\\base_etm.py:108\u001b[0m, in \u001b[0;36mBaseETM._load_word_vectors\u001b[1;34m(self, embeddings_path, embeddings_type, binary_embeddings, headerless_embeddings)\u001b[0m\n\u001b[0;32m    106\u001b[0m embs \u001b[38;5;241m=\u001b[39m pkl\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(embeddings_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m embs:\n\u001b[1;32m--> 108\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[43memb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m()\n\u001b[0;32m    109\u001b[0m     word \u001b[38;5;241m=\u001b[39m line[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mvalues():\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "output = model.train_model(dataset, top_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "td, ch = TopicDiversity(topk=10), Coherence(topk=20, measure = 'c_v') # Initialize metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coherence: \", ch.score(output))\n",
    "print(\"Topic Diversity: \", td.score(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OCTIS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
