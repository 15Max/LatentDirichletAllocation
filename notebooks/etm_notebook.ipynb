{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedded Topic Model (ETM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from octis.preprocessing.preprocessing import Preprocessing\n",
    "from octis.models.ETM import ETM\n",
    "from octis.dataset.dataset import Dataset\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "from octis.optimization.optimizer import Optimizer\n",
    "from skopt.space.space import Real\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "os.chdir(parent_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.embeddings import *\n",
    "from preprocessing.clean_text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the preprocessing \n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_corpus_and_labels_from_songs_csv(csv_input_path = 'data/raw/cleaned_train_lyrics.csv', output_path = 'data/input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = list(nlp.Defaults.stop_words)\n",
    "stopwords_list.extend(CUSTOM_STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessing(lowercase=True,\n",
    "                             min_df = 10,\n",
    "                             max_df = 0.85,\n",
    "                             remove_punctuation=True,\n",
    "                             punctuation=string.punctuation,\n",
    "                             remove_numbers=True,\n",
    "                             lemmatize= True,\n",
    "                             stopword_list=stopwords_list,\n",
    "                             min_chars=3,\n",
    "                             min_words_docs=10,\n",
    "                             language='english',\n",
    "                             split=True,\n",
    "                             verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Checks if dataset folder exists in processed, if not it processes the dataset. Otherwise it loads it\n",
    "if not os.path.exists('data/processed/dataset'):\n",
    "    dataset = preprocessor.preprocess_dataset(documents_path = 'data/input/corpus.txt',labels_path = 'data/input/labels.txt')\n",
    "    dataset.save('data/processed/dataset')\n",
    "else:\n",
    "    dataset = Dataset()\n",
    "    dataset.load_custom_dataset_from_folder('data/processed/dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_file = 'data/input/embeddings.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings not found, generating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5a7b01d23f4505ab3439dfb92699dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  91%|######### | 398M/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_creation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marta\\OneDrive\\Desktop\\LatentDirichletAllocation\\utils\\embeddings.py:60\u001b[0m, in \u001b[0;36mcreate_embeddings\u001b[1;34m(df, embeddings_file, force_creation, batch_size)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, end, batch_size):\n\u001b[0;32m     59\u001b[0m     batch_texts \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlyrics\u001b[39m\u001b[38;5;124m'\u001b[39m][i:i\u001b[38;5;241m+\u001b[39mbatch_size]\u001b[38;5;241m.\u001b[39mtolist()  \u001b[38;5;66;03m# Get batch of texts\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     embeddings_batch \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend(embeddings_batch)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Convert list of batches to numpy array\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marta\\OneDrive\\Desktop\\LatentDirichletAllocation\\utils\\embeddings.py:30\u001b[0m, in \u001b[0;36mget_embeddings_batch\u001b[1;34m(texts, tokenizer, model, device)\u001b[0m\n\u001b[0;32m     27\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(texts, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Perform forward pass on the GPU\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Move the output back to CPU for further processing or storage\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\marta\\miniconda3\\envs\\OCTIS\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\marta\\miniconda3\\envs\\OCTIS\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marta\\miniconda3\\envs\\OCTIS\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1141\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1141\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1153\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1154\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marta\\miniconda3\\envs\\OCTIS\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\marta\\miniconda3\\envs\\OCTIS\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marta\\miniconda3\\envs\\OCTIS\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:694\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    683\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    684\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    685\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    691\u001b[0m         output_attentions,\n\u001b[0;32m    692\u001b[0m     )\n\u001b[0;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 694\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    704\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\marta\\miniconda3\\envs\\OCTIS\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\marta\\miniconda3\\envs\\OCTIS\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marta\\miniconda3\\envs\\OCTIS\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:626\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    623\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    624\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 626\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marta\\miniconda3\\envs\\OCTIS\\lib\\site-packages\\transformers\\pytorch_utils.py:239\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marta\\miniconda3\\envs\\OCTIS\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:638\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m--> 638\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    639\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\marta\\miniconda3\\envs\\OCTIS\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\marta\\miniconda3\\envs\\OCTIS\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marta\\miniconda3\\envs\\OCTIS\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    538\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[1;32m--> 539\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_act_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\marta\\miniconda3\\envs\\OCTIS\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\marta\\miniconda3\\envs\\OCTIS\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marta\\miniconda3\\envs\\OCTIS\\lib\\site-packages\\transformers\\activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embeddings = create_embeddings(df, embeddings_file, force_creation=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_documents': 2225,\n",
       " 'words_document_mean': 120.12,\n",
       " 'vocabulary_length': 2949,\n",
       " 'last-training-doc': 1557,\n",
       " 'last-validation-doc': 1891,\n",
       " 'preprocessing-info': 'Steps:\\n  remove_punctuation\\n  lemmatization\\n  remove_stopwords\\n  filter_words\\n  remove_docs\\nParameters:\\n  removed words with less than 0.005 or more than 0.35 documents with an occurrence of the word in corpus\\n  removed documents with less than 5 words',\n",
       " 'info': {'name': 'BBC_News'},\n",
       " 'labels': ['business', 'entertainment', 'politics', 'sport', 'tech'],\n",
       " 'total_labels': 5}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETM model without optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOPICS = 5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = ETM(num_topics = 10, vocab_size=3000, t_hidden_size=800, theta_act = 'relu', embeddings = None, train_embeddings = True, enc_drop = 0.0, rho_size= 5, emb_size= 10)7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ETM(num_topics= N_TOPICS,  \n",
    "        num_epochs=100, \n",
    "        t_hidden_size=800, \n",
    "        rho_size=384, \n",
    "        embedding_size=384, \n",
    "        activation='relu', \n",
    "        dropout=0.5, \n",
    "        lr=0.005, \n",
    "        optimizer='adam', \n",
    "        batch_size=128, \n",
    "        clip=0.0, \n",
    "        wdecay=1.2e-6, \n",
    "        bow_norm=1, \n",
    "        device=device, \n",
    "        train_embeddings=False, \n",
    "        embeddings_path=embeddings_file, \n",
    "        embeddings_type='pickle', \n",
    "        binary_embeddings=True, \n",
    "        headerless_embeddings=False, \n",
    "        use_partitions=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.5, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=2949, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=20, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=2949, out_features=800, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=800, out_features=20, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=800, out_features=20, bias=True)\n",
      ")\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 929.83 .. NELBO: 929.89\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 209.53 .. NELBO: 209.54\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (inf --> 879.917175).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 879.82 .. NELBO: 879.89\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 207.72 .. NELBO: 207.76\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (879.917175 --> 872.844543).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 874.59 .. NELBO: 874.64\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 207.33 .. NELBO: 207.33\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (872.844543 --> 871.273071).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 873.69 .. NELBO: 873.71\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 207.15 .. NELBO: 207.15\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (871.273071 --> 870.192078).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 873.43 .. NELBO: 873.44\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 207.08 .. NELBO: 207.08\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (870.192078 --> 869.987305).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 873.32 .. NELBO: 873.32\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 207.09 .. NELBO: 207.09\n",
      "****************************************************************************************************\n",
      "EarlyStopping counter: 1 out of 5\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 873.23 .. NELBO: 873.23\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 207.07 .. NELBO: 207.07\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (869.987305 --> 869.947754).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 873.23 .. NELBO: 873.23\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 207.07 .. NELBO: 207.07\n",
      "****************************************************************************************************\n",
      "EarlyStopping counter: 1 out of 5\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 873.23 .. NELBO: 873.23\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 207.06 .. NELBO: 207.06\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (869.947754 --> 869.946716).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 873.2 .. NELBO: 873.21\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 207.06 .. NELBO: 207.06\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (869.946716 --> 869.936401).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 873.17 .. NELBO: 873.19\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 207.06 .. NELBO: 207.06\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (869.936401 --> 869.918579).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 873.15 .. NELBO: 873.19\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 207.05 .. NELBO: 207.05\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (869.918579 --> 869.908081).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 873.13 .. NELBO: 873.17\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 207.05 .. NELBO: 207.06\n",
      "****************************************************************************************************\n",
      "EarlyStopping counter: 1 out of 5\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 873.07 .. NELBO: 873.13\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 207.03 .. NELBO: 207.04\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (869.908081 --> 869.874939).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 873.02 .. NELBO: 873.11\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 207.02 .. NELBO: 207.04\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (869.874939 --> 869.847717).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.13 .. Rec_loss: 872.95 .. NELBO: 873.08\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 207.0 .. NELBO: 207.02\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (869.847717 --> 869.795227).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.2 .. Rec_loss: 872.78 .. NELBO: 872.98\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 206.96 .. NELBO: 206.99\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (869.795227 --> 869.745850).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.26 .. Rec_loss: 872.63 .. NELBO: 872.89\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.06 .. Rec_loss: 206.88 .. NELBO: 206.94\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (869.745850 --> 869.574829).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 872.06 .. NELBO: 872.58\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 206.75 .. NELBO: 206.84\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (869.574829 --> 869.188293).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.78 .. Rec_loss: 870.87 .. NELBO: 871.65\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.21 .. Rec_loss: 206.24 .. NELBO: 206.45\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (869.188293 --> 867.607300).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 1.77 .. Rec_loss: 867.75 .. NELBO: 869.52\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.3 .. Rec_loss: 205.66 .. NELBO: 205.96\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (867.607300 --> 865.720886).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 2.15 .. Rec_loss: 865.92 .. NELBO: 868.07\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.37 .. Rec_loss: 205.23 .. NELBO: 205.6\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (865.720886 --> 864.315063).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 2.58 .. Rec_loss: 863.68 .. NELBO: 866.26\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.41 .. Rec_loss: 204.84 .. NELBO: 205.25\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (864.315063 --> 862.883972).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 2.41 .. Rec_loss: 863.04 .. NELBO: 865.45\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 204.45 .. NELBO: 204.97\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (862.883972 --> 861.782471).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 3.02 .. Rec_loss: 861.3 .. NELBO: 864.32\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.52 .. Rec_loss: 204.26 .. NELBO: 204.78\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (861.782471 --> 860.803589).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 2.93 .. Rec_loss: 861.01 .. NELBO: 863.94\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.56 .. Rec_loss: 204.01 .. NELBO: 204.57\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (860.803589 --> 859.978821).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 3.24 .. Rec_loss: 859.72 .. NELBO: 862.96\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.59 .. Rec_loss: 203.87 .. NELBO: 204.46\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (859.978821 --> 859.655579).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 3.06 .. Rec_loss: 859.81 .. NELBO: 862.87\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.71 .. Rec_loss: 203.69 .. NELBO: 204.4\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (859.655579 --> 859.224365).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 3.61 .. Rec_loss: 858.64 .. NELBO: 862.25\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.74 .. Rec_loss: 203.65 .. NELBO: 204.39\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (859.224365 --> 859.114075).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 3.01 .. Rec_loss: 859.11 .. NELBO: 862.12\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.73 .. Rec_loss: 203.61 .. NELBO: 204.34\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (859.114075 --> 858.753418).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 3.25 .. Rec_loss: 858.66 .. NELBO: 861.91\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.75 .. Rec_loss: 203.43 .. NELBO: 204.18\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (858.753418 --> 857.951233).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 3.48 .. Rec_loss: 857.99 .. NELBO: 861.47\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.74 .. Rec_loss: 203.34 .. NELBO: 204.08\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (857.951233 --> 857.499390).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 3.81 .. Rec_loss: 857.23 .. NELBO: 861.04\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.77 .. Rec_loss: 203.29 .. NELBO: 204.06\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (857.499390 --> 857.307556).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 3.83 .. Rec_loss: 856.38 .. NELBO: 860.21\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.73 .. Rec_loss: 203.16 .. NELBO: 203.89\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (857.307556 --> 856.581543).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 3.74 .. Rec_loss: 856.74 .. NELBO: 860.48\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.74 .. Rec_loss: 203.05 .. NELBO: 203.79\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (856.581543 --> 856.209106).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 3.74 .. Rec_loss: 856.34 .. NELBO: 860.08\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.76 .. Rec_loss: 202.91 .. NELBO: 203.67\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (856.209106 --> 855.626770).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 4.19 .. Rec_loss: 854.51 .. NELBO: 858.7\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.71 .. Rec_loss: 202.76 .. NELBO: 203.47\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (855.626770 --> 854.913208).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 3.69 .. Rec_loss: 855.7 .. NELBO: 859.39\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.74 .. Rec_loss: 202.59 .. NELBO: 203.33\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (854.913208 --> 854.031860).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 3.94 .. Rec_loss: 855.14 .. NELBO: 859.08\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.7 .. Rec_loss: 202.39 .. NELBO: 203.09\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (854.031860 --> 852.975647).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 4.02 .. Rec_loss: 853.66 .. NELBO: 857.68\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.78 .. Rec_loss: 202.23 .. NELBO: 203.01\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (852.975647 --> 852.577820).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 4.42 .. Rec_loss: 852.66 .. NELBO: 857.08\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.86 .. Rec_loss: 201.84 .. NELBO: 202.7\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (852.577820 --> 851.408203).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 4.76 .. Rec_loss: 851.86 .. NELBO: 856.62\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.77 .. Rec_loss: 201.72 .. NELBO: 202.49\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (851.408203 --> 850.617798).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 4.52 .. Rec_loss: 851.12 .. NELBO: 855.64\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.88 .. Rec_loss: 201.48 .. NELBO: 202.36\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (850.617798 --> 849.805176).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 4.99 .. Rec_loss: 850.32 .. NELBO: 855.31\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.86 .. Rec_loss: 201.27 .. NELBO: 202.13\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (849.805176 --> 849.174683).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 4.81 .. Rec_loss: 849.78 .. NELBO: 854.59\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.99 .. Rec_loss: 200.97 .. NELBO: 201.96\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (849.174683 --> 848.115845).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 5.57 .. Rec_loss: 848.33 .. NELBO: 853.9\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.91 .. Rec_loss: 200.91 .. NELBO: 201.82\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (848.115845 --> 847.734863).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 5.15 .. Rec_loss: 848.02 .. NELBO: 853.17\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 1.0 .. Rec_loss: 200.69 .. NELBO: 201.69\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (847.734863 --> 846.890930).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 5.41 .. Rec_loss: 848.22 .. NELBO: 853.63\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.93 .. Rec_loss: 200.62 .. NELBO: 201.55\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (846.890930 --> 846.341919).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 5.22 .. Rec_loss: 848.24 .. NELBO: 853.46\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 1.16 .. Rec_loss: 200.35 .. NELBO: 201.51\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (846.341919 --> 846.000793).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 5.85 .. Rec_loss: 846.87 .. NELBO: 852.72\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.99 .. Rec_loss: 200.4 .. NELBO: 201.39\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (846.000793 --> 845.796936).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->51 .. LR: 0.005 .. KL_theta: 5.65 .. Rec_loss: 846.73 .. NELBO: 852.38\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 1.05 .. Rec_loss: 200.2 .. NELBO: 201.25\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (845.796936 --> 844.964783).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->52 .. LR: 0.005 .. KL_theta: 5.96 .. Rec_loss: 846.46 .. NELBO: 852.42\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 1.02 .. Rec_loss: 200.19 .. NELBO: 201.21\n",
      "****************************************************************************************************\n",
      "EarlyStopping counter: 1 out of 5\n",
      "****************************************************************************************************\n",
      "Epoch----->53 .. LR: 0.005 .. KL_theta: 5.95 .. Rec_loss: 844.47 .. NELBO: 850.42\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 1.08 .. Rec_loss: 199.89 .. NELBO: 200.97\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (844.964783 --> 844.222534).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->54 .. LR: 0.005 .. KL_theta: 6.21 .. Rec_loss: 843.84 .. NELBO: 850.05\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 1.16 .. Rec_loss: 199.69 .. NELBO: 200.85\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (844.222534 --> 843.799072).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->55 .. LR: 0.005 .. KL_theta: 6.41 .. Rec_loss: 843.68 .. NELBO: 850.09\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 1.19 .. Rec_loss: 199.68 .. NELBO: 200.87\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (843.799072 --> 843.787537).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->56 .. LR: 0.005 .. KL_theta: 6.44 .. Rec_loss: 843.08 .. NELBO: 849.52\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 1.2 .. Rec_loss: 199.66 .. NELBO: 200.86\n",
      "****************************************************************************************************\n",
      "EarlyStopping counter: 1 out of 5\n",
      "****************************************************************************************************\n",
      "Epoch----->57 .. LR: 0.005 .. KL_theta: 6.31 .. Rec_loss: 844.61 .. NELBO: 850.92\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 1.07 .. Rec_loss: 199.7 .. NELBO: 200.77\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (843.787537 --> 843.531372).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->58 .. LR: 0.005 .. KL_theta: 6.34 .. Rec_loss: 844.85 .. NELBO: 851.19\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 1.11 .. Rec_loss: 199.65 .. NELBO: 200.76\n",
      "****************************************************************************************************\n",
      "EarlyStopping counter: 1 out of 5\n",
      "****************************************************************************************************\n",
      "Epoch----->59 .. LR: 0.005 .. KL_theta: 6.32 .. Rec_loss: 844.29 .. NELBO: 850.61\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 1.12 .. Rec_loss: 199.57 .. NELBO: 200.69\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (843.531372 --> 843.173218).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->60 .. LR: 0.005 .. KL_theta: 6.55 .. Rec_loss: 842.5 .. NELBO: 849.05\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 1.11 .. Rec_loss: 199.59 .. NELBO: 200.7\n",
      "****************************************************************************************************\n",
      "EarlyStopping counter: 1 out of 5\n",
      "****************************************************************************************************\n",
      "Epoch----->61 .. LR: 0.005 .. KL_theta: 6.62 .. Rec_loss: 843.51 .. NELBO: 850.13\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 1.16 .. Rec_loss: 199.39 .. NELBO: 200.55\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (843.173218 --> 842.685120).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->62 .. LR: 0.005 .. KL_theta: 6.2 .. Rec_loss: 843.33 .. NELBO: 849.53\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 1.13 .. Rec_loss: 199.36 .. NELBO: 200.49\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (842.685120 --> 842.059387).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->63 .. LR: 0.005 .. KL_theta: 6.97 .. Rec_loss: 841.44 .. NELBO: 848.41\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 1.14 .. Rec_loss: 199.7 .. NELBO: 200.84\n",
      "****************************************************************************************************\n",
      "EarlyStopping counter: 1 out of 5\n",
      "****************************************************************************************************\n",
      "Epoch----->64 .. LR: 0.005 .. KL_theta: 6.71 .. Rec_loss: 842.83 .. NELBO: 849.54\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 1.25 .. Rec_loss: 199.29 .. NELBO: 200.54\n",
      "****************************************************************************************************\n",
      "EarlyStopping counter: 2 out of 5\n",
      "****************************************************************************************************\n",
      "Epoch----->65 .. LR: 0.005 .. KL_theta: 7.1 .. Rec_loss: 840.51 .. NELBO: 847.61\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 1.23 .. Rec_loss: 199.42 .. NELBO: 200.65\n",
      "****************************************************************************************************\n",
      "EarlyStopping counter: 3 out of 5\n",
      "****************************************************************************************************\n",
      "Epoch----->66 .. LR: 0.005 .. KL_theta: 6.91 .. Rec_loss: 841.07 .. NELBO: 847.98\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 1.1 .. Rec_loss: 199.36 .. NELBO: 200.46\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (842.059387 --> 842.006226).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->67 .. LR: 0.005 .. KL_theta: 7.26 .. Rec_loss: 840.54 .. NELBO: 847.8\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 1.12 .. Rec_loss: 199.4 .. NELBO: 200.52\n",
      "****************************************************************************************************\n",
      "EarlyStopping counter: 1 out of 5\n",
      "****************************************************************************************************\n",
      "Epoch----->68 .. LR: 0.005 .. KL_theta: 6.79 .. Rec_loss: 842.23 .. NELBO: 849.02\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 1.29 .. Rec_loss: 199.24 .. NELBO: 200.53\n",
      "****************************************************************************************************\n",
      "EarlyStopping counter: 2 out of 5\n",
      "****************************************************************************************************\n",
      "Epoch----->69 .. LR: 0.005 .. KL_theta: 7.17 .. Rec_loss: 840.65 .. NELBO: 847.82\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 1.17 .. Rec_loss: 199.34 .. NELBO: 200.51\n",
      "****************************************************************************************************\n",
      "EarlyStopping counter: 3 out of 5\n",
      "****************************************************************************************************\n",
      "Epoch----->70 .. LR: 0.005 .. KL_theta: 6.9 .. Rec_loss: 841.45 .. NELBO: 848.35\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 1.39 .. Rec_loss: 199.25 .. NELBO: 200.64\n",
      "****************************************************************************************************\n",
      "EarlyStopping counter: 4 out of 5\n",
      "****************************************************************************************************\n",
      "Epoch----->71 .. LR: 0.005 .. KL_theta: 7.32 .. Rec_loss: 840.11 .. NELBO: 847.43\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 1.21 .. Rec_loss: 199.19 .. NELBO: 200.4\n",
      "****************************************************************************************************\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "output = model.train_model(dataset, top_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "td, ch = TopicDiversity(topk=10), Coherence(topk=20, measure = 'c_v') # Initialize metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence:  0.48960463965790246\n",
      "Topic Diversity:  0.28\n"
     ]
    }
   ],
   "source": [
    "print(\"Coherence: \", ch.score(output))\n",
    "print(\"Topic Diversity: \", td.score(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OCTIS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
