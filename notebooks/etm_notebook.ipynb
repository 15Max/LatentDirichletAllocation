{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from octis.models.ETM import ETM\n",
    "from octis.dataset.dataset import Dataset\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "from octis.optimization.optimizer import Optimizer\n",
    "from skopt.space.space import Real\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "os.chdir(parent_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.embeddings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'data/raw/cleaned_train_lyrics.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "df = df.drop(columns = ['Unnamed: 0'])\n",
    "df = df.rename(columns = {'Lyric':'lyrics'})\n",
    "df = df.sample(frac=0.001).reset_index(drop=True) # Uncomment this line to sample a fraction of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings_file = 'data/input/bert_embeddings.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT embeddings not found, generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marta\\miniconda3\\envs\\OCTIS\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "BERT embeddings saved to data/input/bert_embeddings.pkl\n"
     ]
    }
   ],
   "source": [
    "bert_embeddings = create_embeddings(df, bert_embeddings_file, force_creation=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()  \n",
    "dataset.fetch_dataset(\"BBC_News\")\n",
    "#dataset.load_custom_dataset_from_folder(\"data/processed\") # Our custom preprocessed dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_documents': 2225,\n",
       " 'words_document_mean': 120.12,\n",
       " 'vocabulary_length': 2949,\n",
       " 'last-training-doc': 1557,\n",
       " 'last-validation-doc': 1891,\n",
       " 'preprocessing-info': 'Steps:\\n  remove_punctuation\\n  lemmatization\\n  remove_stopwords\\n  filter_words\\n  remove_docs\\nParameters:\\n  removed words with less than 0.005 or more than 0.35 documents with an occurrence of the word in corpus\\n  removed documents with less than 5 words',\n",
       " 'info': {'name': 'BBC_News'},\n",
       " 'labels': ['business', 'entertainment', 'politics', 'sport', 'tech'],\n",
       " 'total_labels': 5}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = ETM(num_topics = 10, vocab_size=3000, t_hidden_size=800, theta_act = 'relu', embeddings = None, train_embeddings = True, enc_drop = 0.0, rho_size= 5, emb_size= 10)7\n",
    "model = ETM(num_topics= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.5, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=2949, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=20, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=2949, out_features=800, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=800, out_features=20, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=800, out_features=20, bias=True)\n",
      ")\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 929.79 .. NELBO: 929.82\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 209.53 .. NELBO: 209.54\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (inf --> 879.885620).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.04 .. Rec_loss: 879.95 .. NELBO: 879.99\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 207.74 .. NELBO: 207.75\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (879.885620 --> 872.789307).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 874.59 .. NELBO: 874.6\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 207.34 .. NELBO: 207.34\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (872.789307 --> 871.331665).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 873.74 .. NELBO: 873.76\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 207.15 .. NELBO: 207.15\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (871.331665 --> 870.197693).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 873.42 .. NELBO: 873.43\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 207.08 .. NELBO: 207.08\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (870.197693 --> 869.989868).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 873.29 .. NELBO: 873.29\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 207.09 .. NELBO: 207.09\n",
      "****************************************************************************************************\n",
      "EarlyStopping counter: 1 out of 5\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 873.24 .. NELBO: 873.24\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 207.07 .. NELBO: 207.07\n",
      "****************************************************************************************************\n",
      "Validation loss decreased (869.989868 --> 869.924561).  Saving model ...\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 873.24 .. NELBO: 873.24\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 207.07 .. NELBO: 207.07\n",
      "****************************************************************************************************\n",
      "EarlyStopping counter: 1 out of 5\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 873.22 .. NELBO: 873.22\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 207.07 .. NELBO: 207.07\n",
      "****************************************************************************************************\n",
      "EarlyStopping counter: 2 out of 5\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 873.2 .. NELBO: 873.2\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 207.07 .. NELBO: 207.07\n",
      "****************************************************************************************************\n",
      "EarlyStopping counter: 3 out of 5\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 873.19 .. NELBO: 873.19\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 207.07 .. NELBO: 207.07\n",
      "****************************************************************************************************\n",
      "EarlyStopping counter: 4 out of 5\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 873.2 .. NELBO: 873.2\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "VALIDATION .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 207.07 .. NELBO: 207.07\n",
      "****************************************************************************************************\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "output = model.train_model(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "td, ch = TopicDiversity(topk=10), Coherence(topk=10, measure = 'c_v') # Initialize metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence:  0.429913503131782\n",
      "Topic Diversity:  0.07\n"
     ]
    }
   ],
   "source": [
    "print(\"Coherence: \", ch.score(output))\n",
    "print(\"Topic Diversity: \", td.score(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OCTIS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
