{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedded Topic Model (ETM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicovis/anaconda3/envs/OCTIS/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import string\n",
    "from octis.preprocessing.preprocessing import Preprocessing\n",
    "from octis.models.ETM import ETM\n",
    "from octis.dataset.dataset import Dataset\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "from octis.optimization.optimizer import Optimizer\n",
    "from skopt.space.space import Real\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from wordcloud import WordCloud\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "os.chdir(parent_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL = False\n",
    "\n",
    "if(SMALL):\n",
    "    data_path = 'data/input_small'\n",
    "    corpus_path = 'data/input_small/corpus.txt'\n",
    "    label_path = 'data/input_small/labels.txt'\n",
    "    embs_path = 'data/input_small/embeddings.pkl'\n",
    "    proc_path = 'data/processed_small/dataset'\n",
    "else:\n",
    "    data_path = 'data/input'\n",
    "    corpus_path = 'data/input/corpus.txt'\n",
    "    label_path = 'data/input/labels.txt'\n",
    "    embs_path = 'data/input/embeddings.pkl'\n",
    "    proc_path = 'data/processed/dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicovis/anaconda3/envs/OCTIS/lib/python3.9/site-packages/dask/dataframe/_pyarrow_compat.py:15: FutureWarning: Minimal version of pyarrow will soon be increased to 14.0.1. You are using 11.0.0. Please consider upgrading.\n",
      "  warnings.warn(\n",
      "/home/nicovis/anaconda3/envs/OCTIS/lib/python3.9/site-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from utils.embeddings import *\n",
    "from preprocessing.clean_text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vectors:  17%|█▋        | 345675/2000000 [00:18<01:00, 27435.91it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test = load_vectors('model/cc.en.300.vec/cc.en.300.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_corpus_and_labels_from_songs_csv(csv_input_path = 'data/raw/cleaned_train_lyrics.csv', output_path = data_path,frac=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft preprocesing & Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_embeddings(corpus_path, embs_path, dim = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_pickle(embs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = {}\n",
    "\n",
    "for emb in embeddings:\n",
    "            line = emb.split()\n",
    "            word = line[0]\n",
    "            vect = np.array(line[1:]).astype(float)\n",
    "            vectors[word] = vect\n",
    "\n",
    "print(len(vectors['the']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessing(lowercase=True,\n",
    "                             min_df = 20,\n",
    "                             max_df = 0.80,\n",
    "                             remove_punctuation=True,\n",
    "                             punctuation=string.punctuation,\n",
    "                             remove_numbers=True,\n",
    "                             lemmatize= True,\n",
    "                             stopword_list=CUSTOM_STOPWORDS,\n",
    "                             min_chars=3,\n",
    "                             min_words_docs=10,\n",
    "                             language='english',\n",
    "                             split=True,\n",
    "                             verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Checks if dataset folder exists in processed, if not it processes the dataset. Otherwise it loads it\n",
    "if not os.path.exists(proc_path):\n",
    "    dataset = preprocessor.preprocess_dataset(documents_path = corpus_path,labels_path = label_path)\n",
    "    dataset.save(proc_path)\n",
    "else:\n",
    "    dataset = Dataset()\n",
    "    dataset.load_custom_dataset_from_folder(proc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETM model without optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOPICS = 10\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "current_device = torch.cuda.current_device()\n",
    "print(f\"Currently using GPU: {current_device}\")\n",
    "print(f\"GPU Name: {torch.cuda.get_device_name(current_device)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ETM(num_topics= N_TOPICS,  \n",
    "        num_epochs=10, \n",
    "        t_hidden_size=400, \n",
    "        rho_size=100, \n",
    "        embedding_size=100,\n",
    "        activation='relu', \n",
    "        dropout=0.5, \n",
    "        lr=0.005, \n",
    "        optimizer='adam', \n",
    "        batch_size=64, \n",
    "        clip=0.0, \n",
    "        wdecay=1.2e-6, \n",
    "        bow_norm=1, \n",
    "        device=device, \n",
    "        train_embeddings=False, \n",
    "        embeddings_path= embs_path,\n",
    "        embeddings_type='pickle', \n",
    "        binary_embeddings=True, \n",
    "        headerless_embeddings=False, \n",
    "        use_partitions=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.train_model(dataset, top_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "td, ch = TopicDiversity(topk=10), Coherence(texts = dataset.get_corpus(), topk=20, measure = 'c_npmi') # Initialize metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coherence: \", ch.score(output))\n",
    "print(\"Topic Diversity: \", td.score(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OCTIS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
